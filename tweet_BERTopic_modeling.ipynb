{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaisyXinyiHe/sentiment_analysis_topic_model/blob/main/tweet_BERTopic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p1YHEkpFyiP"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic\n",
        "\n",
        "!pip install hdbscan\n",
        "!pip install flair\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcFk7ZRoon6q"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brpiAKfzG08T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import emodict ## This is a emoji dictionary downloaded from github and stored in folders\n",
        "# from summarizer import Summarizer,TransformerSummarizer\n",
        "\n",
        "## Import emoji libraries\n",
        "EMOTICONS = emodict.EMOTICONS_EMO\n",
        "UNICODE_EMO = emodict.UNICODE_EMOJI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV662xvZHUcF"
      },
      "outputs": [],
      "source": [
        "tweet_filename1 = 'tweets_2022-02-14.csv'\n",
        "tweets = pd.read_csv(tweet_filename1)\n",
        "tweets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_filename2 = 'tweets_2022-02-12.csv'\n",
        "tweets2 = pd.read_csv(tweet_filename2)\n",
        "tweets = pd.concat([tweets, tweets2])\n",
        "tweets"
      ],
      "metadata": {
        "id": "7yNxIH8kT-4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_date(date_time):\n",
        "  date_time = date_time.split(' ')\n",
        "  return date_time[0]"
      ],
      "metadata": {
        "id": "hk5BWynqACpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['date'] = tweets.datetime.apply(get_date)"
      ],
      "metadata": {
        "id": "JZbHjYlkJYDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[[ 'tweet', 'date']].groupby(['tweet','date']).size().nlargest(10)"
      ],
      "metadata": {
        "id": "TbrawkIiIe0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNKQ82mbHXqV"
      },
      "outputs": [],
      "source": [
        "## Tweet cleaning functions\n",
        "\n",
        "## Change all tweets to lowercase\n",
        "def lower_case(tweet):\n",
        "  return tweet.lower()\n",
        "\n",
        "## Remove punctuation\n",
        "def remove_punctuation(tweet):\n",
        "  PUNCT_TO_REMOVE = string.punctuation+'’'+'「'+'」'\n",
        "  return tweet.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "## Filter stop words\n",
        "def filter_stopwords(tweet):\n",
        "  filtered = ''\n",
        "  stop_words = stopwords.words('english')\n",
        "  stop_words.append('im')\n",
        "  stop_words = set(stop_words)\n",
        "  word_tokens = word_tokenize(tweet)\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  filtered = ' '\n",
        "  filtered =filtered.join(filtered_sentence)\n",
        "  return filtered\n",
        "\n",
        "\n",
        "## Remove unnecessary symbols\n",
        "def clean(tweet):\n",
        "  # tweet = \" \".join(filter(lambda x:x[0]!='@', tweet.split())) # remove @mention\n",
        "  tweet = re.sub(r'@','', tweet) # remove @ symbol\n",
        "  tweet = re.sub(r'#','', tweet) # remove hastag symbol\n",
        "  tweet = re.sub(r'https?:\\/\\/\\S+','', tweet) # remove hyperlink\n",
        "  tweet = re.sub(r'rt[\\s]+','', tweet) # remove 'RT'\n",
        "  # tweet = remove_punctuation(tweet)\n",
        "  tweet = filter_stopwords(tweet)\n",
        "  return tweet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## replace emoji and emoticons to words\n",
        "# def convert_emoticons(tweet):\n",
        "#   for emot in EMOTICONS:\n",
        "#     if emot in tweet:\n",
        "#       tweet = tweet.replace(emot, EMOTICONS[emot])\n",
        "#   return tweet\n",
        "\n",
        "def convert_emoji(tweet):\n",
        "  for emo in UNICODE_EMO:\n",
        "    if emo in tweet:\n",
        "      tweet = tweet.replace(emo, UNICODE_EMO[emo])\n",
        "    tweet = tweet.lower()\n",
        "    tweet = tweet.replace(':', '')\n",
        "    # tweet = tweet.replace('_', ' ')\n",
        "  return tweet\n",
        "\n",
        "## Take away the keywords for search in tweets\n",
        "def take_away_keyword(tweet):\n",
        "  for k in search_word:\n",
        "    if k in tweet:\n",
        "      tweet = tweet.replace(k, '')\n",
        "  return tweet\n",
        "\n",
        "## Connect keywords as one word\n",
        "def connect_keyword(tweet, keyword):\n",
        "  keyword_connected = keyword.replace(' ', '_')\n",
        "  if keyword in tweet:\n",
        "    tweet = tweet.replace(keyword, keyword_connected)\n",
        "  return tweet\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jTMxhSNN797"
      },
      "outputs": [],
      "source": [
        "tweets['tweet_processed'] = tweets['tweet'].apply(lower_case)\n",
        "tweets['tweet_processed']  =tweets['tweet'].apply(clean)\n",
        "# tweets['tweet'] = tweets['tweet'].apply(convert_emoji)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = tweets.sort_values(by='datetime')"
      ],
      "metadata": {
        "id": "ETbAPDphIlln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_nodup = tweets.drop(tweets.loc[tweets['tweet'].duplicated()].index)"
      ],
      "metadata": {
        "id": "qTD--b7xHMep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_nodup = tweets_nodup.reset_index(drop=True)\n",
        "empty_tweet = []\n",
        "for t in range(len(tweets_nodup)):\n",
        "  if len(tweets_nodup.tweet.iloc[t])==0:\n",
        "    empty_tweet.append(t)\n",
        "tweets_nodup = tweets_nodup.drop(index = empty_tweet)\n",
        "tweets_nodup = tweets_nodup.reset_index(drop=True)\n",
        "tweets_nodup"
      ],
      "metadata": {
        "id": "lyHdlYebmu1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJdWFbH7PxZT"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTfcjisy6hU-"
      },
      "outputs": [],
      "source": [
        "all_tweets = [x for x in np.array(tweets_nodup.tweet)]\n",
        "dates = tweets_nodup.datetime.apply(get_date)\n",
        "dates = dates.apply(lambda x: pd.Timestamp(x)).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_tweets), len(dates)"
      ],
      "metadata": {
        "id": "OQHPXJWaHUgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puAQAfs-Qvwx"
      },
      "outputs": [],
      "source": [
        "# Load sentence transformer model\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDWg8H1BWQLs"
      },
      "outputs": [],
      "source": [
        "# Create documents embeddings\n",
        "embeddings = sentence_model.encode(all_tweets, show_progress_bar=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM1UzqvCWZTp"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "# Define UMAP model to reduce embeddings dimension\n",
        "umap_model = umap.UMAP(n_neighbors=30,\n",
        "                       n_components=30,\n",
        "                       min_dist=0.0,\n",
        "                       metric='cosine',\n",
        "                       low_memory=False, \n",
        "                       random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2b5a_RQYoGj"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "# Define HDBSCAN model to perform documents clustering\n",
        "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10,\n",
        "                                min_samples=1,\n",
        "                                metric='euclidean',\n",
        "                                cluster_selection_method='eom',\n",
        "                                prediction_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsqBGIblZ0OF"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "# Create BERTopic model\n",
        "topic_model = BERTopic(top_n_words=30,\n",
        "                       n_gram_range=(1,3), \n",
        "                       calculate_probabilities=True,\n",
        "                       umap_model= umap_model,\n",
        "                       hdbscan_model=hdbscan_model,\n",
        "                       #similarity_threshold_merging=0.5,\n",
        "                       verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZbzKOj_ajmK"
      },
      "outputs": [],
      "source": [
        "# Train model, extract topics and probabilities\n",
        "topics, probabilities = topic_model.fit_transform(all_tweets, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzQJhCHkgtd_"
      },
      "outputs": [],
      "source": [
        "sum(topic_model.get_topic_freq().Count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My-4XaAEycO9"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_freq().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bE3U81HzEti"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9adSJFJMjCIl"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5KKmXiRjKOZ"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_representative_docs()"
      ],
      "metadata": {
        "id": "U3AIzYbBFWPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_over_time = topic_model.topics_over_time(all_tweets, topics, dates)"
      ],
      "metadata": {
        "id": "VOZVhItJfYps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics_over_time(topic_over_time, topics=list(np.arange(0,20)))"
      ],
      "metadata": {
        "id": "bBnFSIXNf5pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gVCfl491hH2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic 2: Change minimum topic size"
      ],
      "metadata": {
        "id": "_7lB-Jk4SkRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_date(date_time):\n",
        "  date_time = date_time.split(' ')\n",
        "  return date_time[0]"
      ],
      "metadata": {
        "id": "9O0QCNcIYQZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5JbUJvveQR0"
      },
      "outputs": [],
      "source": [
        "dates = tweets_nodup.datetime.apply(get_date)\n",
        "dates = dates.apply(lambda x: pd.Timestamp(x)).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tweets_nodup.tweet_processed.to_list()"
      ],
      "metadata": {
        "id": "z8-kbdYdSphV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text), len(dates))"
      ],
      "metadata": {
        "id": "m3QqYZl_ahmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import hdbscan\n",
        "from bertopic import BERTopic\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=40)\n",
        "topic_model2 = BERTopic(min_topic_size=60, n_gram_range=(1,3), verbose=True, umap_model=umap_model)\n",
        "topics2, probabilities2 = topic_model2.fit_transform(text)"
      ],
      "metadata": {
        "id": "-S-TsIxFTBEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model2.get_topic_info()"
      ],
      "metadata": {
        "id": "XPF9IobpTZRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model2.visualize_topics()"
      ],
      "metadata": {
        "id": "Ip22Y51YiV7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic2_over_time = topic_model2.topics_over_time(text, topics2, dates)"
      ],
      "metadata": {
        "id": "95KIIVq5UV7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(topic2_over_time.Frequency)"
      ],
      "metadata": {
        "id": "PDS__2QAbVrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model2.visualize_topics_over_time(topic2_over_time, topics=list(np.arange(0,20)))"
      ],
      "metadata": {
        "id": "lgVfdl79V57L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model2.get_representative_docs()"
      ],
      "metadata": {
        "id": "JHHdcUtPZiy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DDXBB61AtH2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Topic Modeling with Class"
      ],
      "metadata": {
        "id": "e4uIrKV9zcgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence"
      ],
      "metadata": {
        "id": "nS9BLmpgzeF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "## Test model\n",
        "sentence = Sentence('Flair is pretty neat!')\n",
        "classifier.predict(sentence)\n",
        "# print sentence with predicted labels\n",
        "print('Sentence above is: ', sentence.labels)"
      ],
      "metadata": {
        "id": "gTLwGV7O06pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tweets_nodup)"
      ],
      "metadata": {
        "id": "DCqbYEh92zZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [Sentence(s) for s in tweets_nodup['tweet']]"
      ],
      "metadata": {
        "id": "rYVF1xI41qm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "id": "9ruJqa-Nkyrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.predict(sentences)"
      ],
      "metadata": {
        "id": "NGcfjXJ41zay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = str(sentences[0].labels[0])\n",
        "num = float(re.findall(\"\\d+\\.\\d+\", sent)[0])\n",
        "lab = \" \".join(re.findall(\"[a-zA-Z]+\", sent))\n",
        "print(num, lab)"
      ],
      "metadata": {
        "id": "dCh7bbllkcgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_labels=[]\n",
        "sent_conf = []\n",
        "for s in range(0,len(sentences)):\n",
        "  if sentences[s]:\n",
        "    sent = str(sentences[s].labels[0])\n",
        "    sent_conf.append(float(re.findall(\"\\d+\\.\\d+\", sent)[0]))\n",
        "    sent_labels.append(\" \".join(re.findall(\"[a-zA-Z]+\", sent)))\n",
        "  else:\n",
        "    print(s)"
      ],
      "metadata": {
        "id": "qU4fl0FmkiRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_nodup['sentiment'] = sent_labels\n",
        "tweets_nodup['sentiment_confidence'] =sent_conf\n"
      ],
      "metadata": {
        "id": "Emh72vMYkkw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [s for s in tweets_nodup.sentiment]"
      ],
      "metadata": {
        "id": "zhBU5JKTqPCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes.count('POSITIVE'), classes.count('NEGATIVE')"
      ],
      "metadata": {
        "id": "oxsgsVif2VcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(classes), len(text), len(topics2))"
      ],
      "metadata": {
        "id": "kjTplrKuqY54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_per_class = topic_model.topics_per_class(text, topics, classes=classes)"
      ],
      "metadata": {
        "id": "qzIVoxV_kpK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_per_class"
      ],
      "metadata": {
        "id": "9VxVHTAFpmvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics_per_class(topics_per_class)"
      ],
      "metadata": {
        "id": "GAEzciypqu1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text), len(topics2)"
      ],
      "metadata": {
        "id": "s1lpJ_Viq1Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_per_class2 = topic_model2.topics_per_class(text, topics2, classes=classes)"
      ],
      "metadata": {
        "id": "G2AB5XtFrjGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topics2_red, topics2_red_prob = topic_model2.reduce_topics(text, topics2, nr_topics=7)\n",
        "# topic_model2.get_topic_info()"
      ],
      "metadata": {
        "id": "BFcsquVISb23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model2.visualize_topics_per_class(topics_per_class2)"
      ],
      "metadata": {
        "id": "l5K87phQrjGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_processed_tweet = tweets_nodup.tweet"
      ],
      "metadata": {
        "id": "v3CN_J2qLCg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(topics2), len(text), len(non_processed_tweet), len(classes)"
      ],
      "metadata": {
        "id": "FVOwGUWkP8I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_docs = {topic: [] for topic in set(topics2)}\n",
        "for i, (topic, doc, sent, tweet) in enumerate(zip(topics2, text ,classes, non_processed_tweet)):\n",
        "  doc = [i, doc,sent, tweet]\n",
        "  topic_docs[topic].append(doc)"
      ],
      "metadata": {
        "id": "vvgp00DH8IVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "pos_tweet = []\n",
        "neg_tweet = []\n",
        "for t in range(-1, len(topic_docs)-1):\n",
        "  positive_doc = []\n",
        "  negative_doc = []   \n",
        "  for s in topic_docs[t]:\n",
        "    if 'POSITIVE' in s:\n",
        "      positive_doc.append(s[3])\n",
        "    if 'NEGATIVE' in s:\n",
        "      negative_doc.append(s[3])\n",
        "  pos_tweet.append(random.sample(positive_doc, k=1))\n",
        "  neg_tweet.append(random.sample(negative_doc, k=1))\n",
        "  # pos_tweet.append(positive_doc[0])\n",
        "  # neg_tweet.append(negative_doc[0])"
      ],
      "metadata": {
        "id": "o7L0c228CIK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_per_class2_pos = topics_per_class2[['Topic', 'Words', 'Frequency']].loc[topics_per_class2.Class == 'POSITIVE']\n",
        "topics_per_class2_neg = topics_per_class2[['Topic', 'Words', 'Frequency']].loc[topics_per_class2.Class == 'NEGATIVE']\n",
        "all_topics_by_class = topics_per_class2_pos.set_index('Topic').join(topics_per_class2_neg.set_index('Topic'), lsuffix='_pos', rsuffix='_neg', on='Topic')\n",
        "all_topics_by_class['tweet_example_pos'] = pos_tweet\n",
        "all_topics_by_class['tweet_example_neg'] = neg_tweet\n",
        "all_topics_by_class \n"
      ],
      "metadata": {
        "id": "eOiSJphPrjGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_topics_by_class.to_csv('all_topics_by_class.csv')"
      ],
      "metadata": {
        "id": "kfRGvL2WMoKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NfxAAbksUFdI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tweet_BERTopic_modeling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}